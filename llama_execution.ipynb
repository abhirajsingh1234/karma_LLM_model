{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1094f68c-35f2-461a-8223-3e4809f7da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer)\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "import pandas\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e050aa-d509-42c1-ac20-da309c851440",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token=\"your-access-token\"\n",
    "model_name='AbhirajSinghRajpurohit/Llama-3.2-1B-karma-finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5182d277-c526-4a2b-85a4-4c912dd9ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pandas.read_csv('Content_Storage_df.csv')\n",
    "# embedding_df=pandas.DataFrame(data)\n",
    "# embedding_df.head(1)\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# collection = chroma_client.get_or_create_collection(\"karma_embeddings\")\n",
    "\n",
    "# BATCH_SIZE = 166  # ChromaDB's max batch size\n",
    "\n",
    "# def insert_data(df):\n",
    "#     # Ensure embeddings are in the correct format (list of floats)\n",
    "#     df[\"embedding_vector\"] = df[\"embedding_vector\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "#     # Convert verse_id to float format for uniqueness\n",
    "#     df[\"verse_id\"] = df.apply(lambda row: float(f\"{row['chapter_id']}.{row['verse_id']}\"), axis=1)\n",
    "\n",
    "#     # Insert data in batches\n",
    "#     for i in range(0, len(df), BATCH_SIZE):\n",
    "#         batch = df.iloc[i : i + BATCH_SIZE]\n",
    "\n",
    "#         collection.add(\n",
    "#             ids=batch[\"verse_id\"].astype(str).tolist(),      # Convert verse_id to string\n",
    "#             documents=batch[\"verse_text\"].tolist(),          # List of verses\n",
    "#             embeddings=batch[\"embedding_vector\"].tolist()    # List of embeddings\n",
    "#         )\n",
    "#         print(f\"Inserted {len(batch)} records...\")\n",
    "\n",
    "#     print(\"âœ… All data inserted successfully!\")\n",
    "\n",
    "# # Call function to insert your data\n",
    "# insert_data(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299d9c31-d5b9-4e69-8f86-787faa079835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05a00c85d41454e9a32f85d2084ffb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--AbhirajSinghRajpurohit--Llama-3.2-1B-karma-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7491b86ccc422bb13ea19f1cd4db76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e953d63a999464094fd6433899edd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460bf79d56154690a79e84ee99f97efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7318aea8b6bf4a4c811863cc69234dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26acc22cc9a34ddca328884c86ab4bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_collection(\"karma_embeddings\")\n",
    "\n",
    "user_db = 'user_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6feef8-51d5-497f-99ad-2465a1084525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, top_k=1):\n",
    "    embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "    question_embedding = embedding_fn([question])[0]\n",
    "\n",
    "    # Retrieve top-k matching documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    if results[\"documents\"]:\n",
    "        # print(results[\"documents\"])\n",
    "        flat_documents = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "        return \" \".join(flat_documents) if flat_documents else \"No relevant context found.\"\n",
    "    \n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    input_text = f\"Question: {question} Context: {context} Answer:\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # Stops when EOS is generated\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # answer = generated_text.replace(input_text, \"\").strip()\n",
    "    answer = generated_text.split('Answer:')[1]\n",
    "    print()\n",
    "    print('RAW-OUTPUT-GENERATED-BY-MODEL: '+ generated_text)\n",
    "    print()\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43161976-3961-460a-b124-9036799c6046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER :   What should a wise man not do?\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    escapers= ['exit']\n",
    "    question = input('USER : ')\n",
    "    context = retrieve_context(question)\n",
    "    answer= generate_answer(question,context)\n",
    "    final_answer ='EXTRACTED ANSWER FROM MODEL OUTPUT : '+ answer\n",
    "    if question.lower() in escapers:\n",
    "        break\n",
    "    for char in final_answer:\n",
    "        sys.stdout.write(char)  # Write character without newline\n",
    "        sys.stdout.flush()      # Force immediate output\n",
    "        time.sleep(0.1)         # Adjust speed\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613291ad-96f3-4c84-b9af-2169f39689dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8a51b-c56d-4921-9939-72fa0ad15d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
