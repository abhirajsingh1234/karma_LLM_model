{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1094f68c-35f2-461a-8223-3e4809f7da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer)\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "import pandas\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e050aa-d509-42c1-ac20-da309c851440",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token=\"your-access-token\"\n",
    "model_name='AbhirajSinghRajpurohit/Llama-3.2-1B-karma-finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5182d277-c526-4a2b-85a4-4c912dd9ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pandas.read_csv('Content_Storage_df.csv')\n",
    "# embedding_df=pandas.DataFrame(data)\n",
    "# embedding_df.head(1)\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# collection = chroma_client.get_or_create_collection(\"karma_embeddings\")\n",
    "\n",
    "# BATCH_SIZE = 166  # ChromaDB's max batch size\n",
    "\n",
    "# def insert_data(df):\n",
    "#     # Ensure embeddings are in the correct format (list of floats)\n",
    "#     df[\"embedding_vector\"] = df[\"embedding_vector\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "#     # Convert verse_id to float format for uniqueness\n",
    "#     df[\"verse_id\"] = df.apply(lambda row: float(f\"{row['chapter_id']}.{row['verse_id']}\"), axis=1)\n",
    "\n",
    "#     # Insert data in batches\n",
    "#     for i in range(0, len(df), BATCH_SIZE):\n",
    "#         batch = df.iloc[i : i + BATCH_SIZE]\n",
    "\n",
    "#         collection.add(\n",
    "#             ids=batch[\"verse_id\"].astype(str).tolist(),      # Convert verse_id to string\n",
    "#             documents=batch[\"verse_text\"].tolist(),          # List of verses\n",
    "#             embeddings=batch[\"embedding_vector\"].tolist()    # List of embeddings\n",
    "#         )\n",
    "#         print(f\"Inserted {len(batch)} records...\")\n",
    "\n",
    "#     print(\"âœ… All data inserted successfully!\")\n",
    "\n",
    "# # Call function to insert your data\n",
    "# insert_data(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299d9c31-d5b9-4e69-8f86-787faa079835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05a00c85d41454e9a32f85d2084ffb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--AbhirajSinghRajpurohit--Llama-3.2-1B-karma-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7491b86ccc422bb13ea19f1cd4db76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e953d63a999464094fd6433899edd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460bf79d56154690a79e84ee99f97efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7318aea8b6bf4a4c811863cc69234dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26acc22cc9a34ddca328884c86ab4bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_collection(\"karma_embeddings\")\n",
    "\n",
    "user_db = 'user_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6feef8-51d5-497f-99ad-2465a1084525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, top_k=1):\n",
    "    embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "    question_embedding = embedding_fn([question])[0]\n",
    "\n",
    "    # Retrieve top-k matching documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    if results[\"documents\"]:\n",
    "        # print(results[\"documents\"])\n",
    "        flat_documents = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "        return \" \".join(flat_documents) if flat_documents else \"No relevant context found.\"\n",
    "    \n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    input_text = f\"Question: {question} Context: {context} Answer:\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # Stops when EOS is generated\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # answer = generated_text.replace(input_text, \"\").strip()\n",
    "    answer = generated_text.split('Answer:')[1]\n",
    "    print()\n",
    "    print('**RAW-OUTPUT-GENERATED-BY-MODEL** : '+ generated_text)\n",
    "    print()\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43161976-3961-460a-b124-9036799c6046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER :   What should a wise man not do?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW-OUTPUT-GENERATED-BY-MODEL: Question:  What should a wise man not do? Context: th brute force, and seek knowledge with out any effort and prosperity, by working the ruin of others, cannot be called wise it is foolishness to cut dowry a tree for its fruits a tree or a project that yields good fruits, should never be uprooted, o vipra how can i believe that a rich man to be an anchorite, and a drunken woman chaste trust not the untrustworthy nor confide any secret in your friend, lest he might betray you in a fit of anger a vast, deep and childlike faith in all, a universal clemency, and a close and watchful veiling of his own god like inherent virtues, are the traits which mark a noble soul the doer of an act does alone feel its consequence hence, all works should be coolly pondered over before execution the six things, such as the use of a new wine or trimulakam lit,horse radish in its three different forms, the partaking of curd in the night, or of dried meat, sleep during the day, and the bed of an elderly woman, should be abjured a family is a poison ruinous to a poor man a young wife is a poison fatal to an old man poison is an ill acquired knowledge, or a food that cannot be digested sweet is charity to a man of bountiful spirit, sweet is social elevation to a man who has risen from the ranks, bounties are sweet to the indigent, and by far the sweetest of them all is his youthful bride to a man of advanced years exces sive waterdrinking, constant use of hard seats or cushions, loss of vital fluid, repression of any natural urging of the body, sleep by the day and vigils in the night, are the six exciting factors of disease exposure to the rays of the sun when he stays in the sign of virgo, sexual excesses exposure to the smoke of a cremationground, the heating of the palms of ones hands, and the sight of a woman in her menses, tend to shorten life dried meat, exposure to the rays of the autumn, sun in virgo, curd of more than two days manufacture tarunam dadhi, intercourse with a woman older than ones own self, and sleep and coitus in the morning are the six depletive agents that tend to reduce strength and vitality the six things such as, butter manufac tured and clarified very same day, grapes, a young wife, a milk potion, tepid water and the shadow of a tree, instan taneously contribute to the formation of strength in the human organism the water of a well, the shadow of a vata tree, and the breasts of a youthful maiden, become warm in winter and cold in summer the three following, vist a young wife, an annoint ment with oil, and a wholesome, toothsome meal instanta neously tend to impart strength to the organism a fatiguing journey, an act of sexual intercourse and an attack of fever are the three factors which instantaneously diminish the strength of a man dry meat should not be taken with milk, nor a man should sit down to a meal in the company of his friends and wives, or with the king of his country, inasmuch as such a conduct might lead to a rupture and misunderstand ing torn and filthy clothes, voracious eating, rough speak ing and sleep at dusk and dawn, are the factors which may bring bad luck to the god chakrapni, the lord of the wealth goddess the cutting of weeds with nails, the digging of earth with toes, the bandying and beating of legs against each other, the wearing of filthy garments and dirty clo Answer: it is foolishness to cut dowry a tree for its fruits a tree or a project that yields\n",
      "\n",
      "XTRACTED ANSWER FROM MODEL OUTPUT :  it is foolishness to cut dowry a tree for its fruits a tree or a project that yields\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    escapers= ['exit']\n",
    "    question = input('**USER** : ')\n",
    "    context = retrieve_context(question)\n",
    "    answer= generate_answer(question,context)\n",
    "    final_answer ='**EXTRACTED ANSWER FROM MODEL OUTPUT** : '+ answer\n",
    "    if question.lower() in escapers:\n",
    "        break\n",
    "    for char in final_answer:\n",
    "        sys.stdout.write(char)  # Write character without newline\n",
    "        sys.stdout.flush()      # Force immediate output\n",
    "        time.sleep(0.1)         # Adjust speed\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613291ad-96f3-4c84-b9af-2169f39689dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8a51b-c56d-4921-9939-72fa0ad15d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
