{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d654f1-65b6-410b-9321-18bc576198b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes typing_extensions\n",
    "pip install --upgrade bitsandbytes typing_extensions\n",
    "pip install pandas\n",
    "pip install accelerate\n",
    "pip install transformers\n",
    "pip install datasets\n",
    "pip install Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ee079-6f17-4f87-92d2-176d7577f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from accelerate import dispatch_model\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e4ae8-3658-4ddb-9ee3-4d2d0c19720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token=\"your-access-token\"\n",
    "model_name='meta-llama/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea8570-5345-41d6-bb0f-472b215bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c400f5-ca1d-4d32-a90e-833e72fcb466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('subset_1.csv')\n",
    "data = pd.read_csv('qa_pairs.csv')\n",
    "qa_pairs = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# âœ… Step 5: Process Data (Prepare Question-Answer Pairs)\n",
    "def process_data(qa_pairs):\n",
    "    data = []\n",
    "    for _, row in qa_pairs.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        context = row['verse_text']\n",
    "        input_text = f\"question: {question} context: {context}\"\n",
    "        output_text = answer\n",
    "        data.append({'input_text': input_text, 'target_text': output_text})\n",
    "    return data\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     # Format input as: \"Question: ... Context: ... Answer: ...\"\n",
    "#     full_text = [f\"Question: {q} Context: {c} Answer: {a}\" for q, c, a in zip(examples[\"input_text\"], examples[\"context\"], examples[\"target_text\"])]\n",
    "\n",
    "#     # Tokenize the full sequence\n",
    "#     model_inputs = tokenizer(\n",
    "#         full_text,\n",
    "#         max_length=512,  \n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "\n",
    "#     # Shift labels for causal LM loss (ignore padding)\n",
    "#     labels = model_inputs[\"input_ids\"].copy()\n",
    "#     labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n",
    "\n",
    "#     model_inputs[\"labels\"] = labels\n",
    "#     return model_inputs\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate input_text (question + context) and target_text (answer)\n",
    "    full_text = [q + \" \" + a for q, a in zip(examples[\"input_text\"], examples[\"target_text\"])]\n",
    "\n",
    "    # Tokenize the full sequence\n",
    "    model_inputs = tokenizer(\n",
    "        full_text,\n",
    "        max_length=512,  # Ensure it fits within LLaMA context window\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Shift labels for causal LM loss (ignore padding)\n",
    "    labels = model_inputs[\"input_ids\"].copy()\n",
    "    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "training_data = process_data(qa_pairs)\n",
    "\n",
    "# âœ… Step 6: Split Data into Train & Eval Sets\n",
    "train_data, eval_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 8: Tokenize the Dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# âœ… Step 9: Define Training Arguments (Low Memory Usage)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_finetune_new\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # ðŸ”¥ Lower batch size to fit GPU\n",
    "    per_device_eval_batch_size=2,  # ðŸ”¥ Lower batch size\n",
    "    num_train_epochs=3,            #num of times entire data will be passes to model\n",
    "    weight_decay=0.01,             # helps prevent overfitting by penalizing large weights \n",
    "    save_strategy=\"no\",\n",
    "    gradient_accumulation_steps=4,  # ðŸ”¥ Simulates batch size of 4\n",
    "    bf16=True,  # ðŸ”¥ Enables mixed precision training\n",
    "    # logging_steps=50,  # Log training progress every 50 steps\n",
    "    optim = \"adamw_8bit\",\n",
    "    warmup_steps=500,            # gradually increase the learning rate at the start ,helps stabilize the model and prevents large updates that might destabilize training\n",
    "    logging_dir=\"./llama3_logs_new\"\n",
    ")\n",
    "# âœ… Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "# âœ… Ensure all parameters require gradients\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd537be-a89c-4710-a28c-7e16d48faea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    data_collator=data_collator,         #format input correctly\n",
    "    # packing=False       #rows are processed seperatly, saves memory consumes processing\n",
    ")\n",
    "\n",
    "# âœ… Step 11: Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02841934-f5ee-4d88-ba14-38ad9f9096f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token = access_token) \n",
    "model_name1 = \"AbhirajSinghRajpurohit/Llama-3.2-1B-karma-finetuned\"\n",
    "\n",
    "model.push_to_hub(model_name1)\n",
    "tokenizer.push_to_hub(model_name1)\n",
    "\n",
    "print(f\"âœ… Model uploaded to: https://huggingface.co/{model_name1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48902e52-7020-48f2-8f18-72b99362fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step\tTraining Loss\n",
    "# 500\t2.682600\n",
    "# 1000\t0.740300\n",
    "# 1500\t0.262900\n",
    "# 2000\t0.163400\n",
    "# 2500\t0.121700\n",
    "# 3000\t0.089800\n",
    "# 3500\t0.078600\n",
    "# 4000\t0.069800\n",
    "# 4500\t0.061800\n",
    "# 5000\t0.055100\n",
    "# 5500\t0.049200\n",
    "# 6000\t0.044800\n",
    "# 6500\t0.041700\n",
    "# 7000\t0.039200\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
